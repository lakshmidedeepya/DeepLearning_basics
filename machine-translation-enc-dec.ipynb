{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7277fcf9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-30T09:24:10.476198Z",
     "iopub.status.busy": "2025-06-30T09:24:10.475779Z",
     "iopub.status.idle": "2025-06-30T09:24:27.413981Z",
     "shell.execute_reply": "2025-06-30T09:24:27.413245Z"
    },
    "papermill": {
     "duration": 16.942526,
     "end_time": "2025-06-30T09:24:27.415256",
     "exception": false,
     "start_time": "2025-06-30T09:24:10.472730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/200, Loss: 0.4150\n",
      "Epoch 40/200, Loss: 0.0237\n",
      "Epoch 60/200, Loss: 0.0103\n",
      "Epoch 80/200, Loss: 0.0074\n",
      "Epoch 100/200, Loss: 0.0060\n",
      "Epoch 120/200, Loss: 0.0050\n",
      "Epoch 140/200, Loss: 0.0042\n",
      "Epoch 160/200, Loss: 0.0036\n",
      "Epoch 180/200, Loss: 0.0031\n",
      "Epoch 200/200, Loss: 0.0027\n",
      "Decoded token IDs: [3, 4]\n",
      "Predicted Translation: namaste subhodayam\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# ---------------------------------------------\n",
    "# Vocabulary Setup\n",
    "# ---------------------------------------------\n",
    "SRC_VOCAB = {'<pad>': 0, '<sos>': 1, '<eos>': 2, 'hello': 3,'good morning':4}\n",
    "TRG_VOCAB = {'<pad>': 0, '<sos>': 1, '<eos>': 2, 'namaste': 3,'subhodayam':4}\n",
    "\n",
    "SRC_itos = {v: k for k, v in SRC_VOCAB.items()}\n",
    "TRG_itos = {v: k for k, v in TRG_VOCAB.items()}\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Encoder\n",
    "# ---------------------------------------------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        return hidden  # (1, batch, hid_dim)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Decoder\n",
    "# ---------------------------------------------\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        input = input.unsqueeze(1)  # (batch, 1)\n",
    "        embedded = self.embedding(input)  # (batch, 1, emb_dim)\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        prediction = self.fc_out(output.squeeze(1))  # (batch, output_dim)\n",
    "        return prediction, hidden\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Seq2Seq Wrapper with Inference\n",
    "# ---------------------------------------------\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, criterion, teacher_forcing=True):\n",
    "        hidden = self.encoder(src)\n",
    "        input_token = trg[:, 0]  # <sos>\n",
    "        loss = 0\n",
    "        for t in range(1, trg.shape[1]):\n",
    "            output, hidden = self.decoder(input_token, hidden)\n",
    "            loss += criterion(output, trg[:, t])\n",
    "            input_token = trg[:, t] if teacher_forcing else output.argmax(1)\n",
    "        return loss\n",
    "\n",
    "    def inference(self, src, max_len=10):\n",
    "        self.eval()\n",
    "        hidden = self.encoder(src)\n",
    "        input_token = torch.tensor([TRG_VOCAB['<eos>']], device=self.device)\n",
    "\n",
    "        output_tokens = []\n",
    "        for _ in range(max_len):\n",
    "            output, hidden = self.decoder(input_token, hidden)\n",
    "            top1 = output.argmax(1)  # Greedy decoding\n",
    "            if top1.item() == TRG_VOCAB['<eos>']:\n",
    "                break\n",
    "            output_tokens.append(top1.item())\n",
    "            input_token = top1\n",
    "        return output_tokens\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Run Translation\n",
    "# ---------------------------------------------\n",
    "'''device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "encoder = Encoder(INPUT_DIM, EMB_DIM, HID_DIM).to(device)\n",
    "decoder = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM).to(device)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "# Inference\n",
    "src_sentence = src_sentence.to(device)\n",
    "predicted_indices = model(src_sentence)\n",
    "\n",
    "# Convert indices back to words\n",
    "translated_sentence = [TRG_itos[idx] for idx in predicted_indices]\n",
    "print(\"Predicted translation:\", ' '.join(translated_sentence))'''\n",
    "# -------------------------------------\n",
    "# 5. Training and Inference\n",
    "# -------------------------------------\n",
    "def main():\n",
    "    # Parameters\n",
    "    INPUT_DIM = len(SRC_VOCAB)\n",
    "    OUTPUT_DIM = len(TRG_VOCAB)\n",
    "    EMB_DIM = 8\n",
    "    HID_DIM = 16\n",
    "    NUM_EPOCHS = 200\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    encoder = Encoder(INPUT_DIM, EMB_DIM, HID_DIM).to(device)\n",
    "    decoder = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM).to(device)\n",
    "    model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=TRG_VOCAB['<pad>'])\n",
    "\n",
    "    # Input: <sos> hello good morning<eos>\n",
    "    # Target: <sos> namaste subhodayam<eos>\n",
    "    src_tensor = torch.tensor([[SRC_VOCAB['<sos>'], SRC_VOCAB['hello'],SRC_VOCAB['good morning'], SRC_VOCAB['<eos>']]], dtype=torch.long).to(device)\n",
    "    trg_tensor = torch.tensor([[TRG_VOCAB['<sos>'], TRG_VOCAB['namaste'],TRG_VOCAB['subhodayam'], TRG_VOCAB['<eos>']]], dtype=torch.long).to(device)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(src_tensor, trg_tensor, criterion)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Inference\n",
    "    model.eval()\n",
    "    prediction = model.inference(src_tensor)\n",
    "    #print(\"src:\", src_tensor)\n",
    "    print(\"Decoded token IDs:\", prediction)\n",
    "    translated_words = [TRG_itos[i] for i in prediction]\n",
    "    print(\"Predicted Translation:\", ' '.join(translated_words))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 26.255774,
   "end_time": "2025-06-30T09:24:30.294227",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-30T09:24:04.038453",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
