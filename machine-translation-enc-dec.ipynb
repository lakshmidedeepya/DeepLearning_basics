{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64d6920f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-30T08:59:28.297742Z",
     "iopub.status.busy": "2025-06-30T08:59:28.297524Z",
     "iopub.status.idle": "2025-06-30T08:59:38.748897Z",
     "shell.execute_reply": "2025-06-30T08:59:38.748058Z"
    },
    "papermill": {
     "duration": 10.455098,
     "end_time": "2025-06-30T08:59:38.750083",
     "exception": false,
     "start_time": "2025-06-30T08:59:28.294985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/200, Loss: 0.1507\n",
      "Epoch 40/200, Loss: 0.0121\n",
      "Epoch 60/200, Loss: 0.0061\n",
      "Epoch 80/200, Loss: 0.0046\n",
      "Epoch 100/200, Loss: 0.0037\n",
      "Epoch 120/200, Loss: 0.0031\n",
      "Epoch 140/200, Loss: 0.0026\n",
      "Epoch 160/200, Loss: 0.0023\n",
      "Epoch 180/200, Loss: 0.0020\n",
      "Epoch 200/200, Loss: 0.0017\n",
      "Decoded token IDs: []\n",
      "Predicted Translation: \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# ---------------------------------------------\n",
    "# Vocabulary Setup\n",
    "# ---------------------------------------------\n",
    "SRC_VOCAB = {'<pad>': 0, '<sos>': 1, '<eos>': 2, 'hello': 3}\n",
    "TRG_VOCAB = {'<pad>': 0, '<sos>': 1, '<eos>': 2, 'bonjour': 3}\n",
    "\n",
    "SRC_itos = {v: k for k, v in SRC_VOCAB.items()}\n",
    "TRG_itos = {v: k for k, v in TRG_VOCAB.items()}\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Encoder\n",
    "# ---------------------------------------------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        return hidden  # (1, batch, hid_dim)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Decoder\n",
    "# ---------------------------------------------\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        input = input.unsqueeze(1)  # (batch, 1)\n",
    "        embedded = self.embedding(input)  # (batch, 1, emb_dim)\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        prediction = self.fc_out(output.squeeze(1))  # (batch, output_dim)\n",
    "        return prediction, hidden\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Seq2Seq Wrapper with Inference\n",
    "# ---------------------------------------------\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, criterion, teacher_forcing=True):\n",
    "        hidden = self.encoder(src)\n",
    "        input_token = trg[:, 0]  # <sos>\n",
    "        loss = 0\n",
    "        for t in range(1, trg.shape[1]):\n",
    "            output, hidden = self.decoder(input_token, hidden)\n",
    "            loss += criterion(output, trg[:, t])\n",
    "            input_token = trg[:, t] if teacher_forcing else output.argmax(1)\n",
    "        return loss\n",
    "\n",
    "    def inference(self, src, max_len=10):\n",
    "        self.eval()\n",
    "        hidden = self.encoder(src)\n",
    "        input_token = torch.tensor([TRG_VOCAB['<eos>']], device=self.device)\n",
    "\n",
    "        output_tokens = []\n",
    "        for _ in range(max_len):\n",
    "            output, hidden = self.decoder(input_token, hidden)\n",
    "            top1 = output.argmax(1)  # Greedy decoding\n",
    "            if top1.item() == TRG_VOCAB['<eos>']:\n",
    "                break\n",
    "            output_tokens.append(top1.item())\n",
    "            input_token = top1\n",
    "        return output_tokens\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Run Translation\n",
    "# ---------------------------------------------\n",
    "'''device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "encoder = Encoder(INPUT_DIM, EMB_DIM, HID_DIM).to(device)\n",
    "decoder = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM).to(device)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "# Inference\n",
    "src_sentence = src_sentence.to(device)\n",
    "predicted_indices = model(src_sentence)\n",
    "\n",
    "# Convert indices back to words\n",
    "translated_sentence = [TRG_itos[idx] for idx in predicted_indices]\n",
    "print(\"Predicted translation:\", ' '.join(translated_sentence))'''\n",
    "# -------------------------------------\n",
    "# 5. Training and Inference\n",
    "# -------------------------------------\n",
    "def main():\n",
    "    # Parameters\n",
    "    INPUT_DIM = len(SRC_VOCAB)\n",
    "    OUTPUT_DIM = len(TRG_VOCAB)\n",
    "    EMB_DIM = 8\n",
    "    HID_DIM = 16\n",
    "    NUM_EPOCHS = 200\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    encoder = Encoder(INPUT_DIM, EMB_DIM, HID_DIM).to(device)\n",
    "    decoder = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM).to(device)\n",
    "    model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=TRG_VOCAB['<pad>'])\n",
    "\n",
    "    # Input: <sos> hello <eos>\n",
    "    # Target: <sos> bonjour <eos>\n",
    "    src_tensor = torch.tensor([[SRC_VOCAB['<sos>'], SRC_VOCAB['hello'], SRC_VOCAB['<eos>']]], dtype=torch.long).to(device)\n",
    "    trg_tensor = torch.tensor([[TRG_VOCAB['<sos>'], TRG_VOCAB['bonjour'], TRG_VOCAB['<eos>']]], dtype=torch.long).to(device)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(src_tensor, trg_tensor, criterion)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Inference\n",
    "    model.eval()\n",
    "    prediction = model.inference(src_tensor)\n",
    "    print(\"Decoded token IDs:\", prediction)\n",
    "    translated_words = [TRG_itos[i] for i in prediction]\n",
    "    print(\"Predicted Translation:\", ' '.join(translated_words))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17.308753,
   "end_time": "2025-06-30T08:59:41.343257",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-30T08:59:24.034504",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
